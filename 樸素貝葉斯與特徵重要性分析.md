# 使用樸素貝葉斯來做特徵工程

## 特徵重要性怎麼分析？
當我們在分析特徵時，經常需要對特徵的重要性做分析。透過特徵重要性分析，我們可以理解哪些特徵對模型的表現存在了決定性的影響。但特徵重要性分析的方法很多，一言難盡，不過從思想上來講我們可以分成三大類：基於統計(filter method)，基於模型表現(wrapped method)，以及基於訓練過程的貢獻(embedding method)。我們來簡要的解釋一下這三種方法的區別：

* 基於統計 :    
基於統計指的就是透過對每個特徵進行一些數值統計來評估特徵的重要性，例如我們可以計算每個特徵的variance，如果variance很低，表示該特徵在不同的樣本上幾乎沒什麼變化，那麼我們就可以認為該特徵無法區別不同的樣本間的差異，這樣的特徵我們就可以認為是不重要的。又或者我們可以去計算特徵與label間的correlation，如果關聯係數偏低，我們也可以認為該特徵可能不重要。基於統計的方法最大的優勢在於我們不需要進行任何模型訓練，僅僅從特徵的數據分佈就可以做出估計，成本低，也有助於我們再進一步的動作之前先對對特徵進行理解。

* 基於模型表現：    
基於模型表現指的是對直接評估特徵對模型的表現看哪個好來決定特徵重要性。例如我們可以先選出一批最符合業務直覺的特徵當作基礎，再把特徵一個一個添加（或移除）重新訓練，看哪些特徵添加進去後對模型的表現提升最好。又或者我們可以先訓練好一個模型，再把測試集中的特徵隨機的打亂看看哪個對模型的影響最大(permutation feature importance)等等。種類繁多，此處就不一一列舉了。

* 基於訓練過程的貢獻：  
某些模型，你在訓練完成後就會天然的產生出特徵重要性。最常見的就是線性迴歸或是邏輯迴歸，當模型訓練完成了，你就會得到每個特徵的線性展開係數，這本質上就是一種特徵重要性的評估，又例如樹模型，你在訓練的過程中自然的就會得出每個特徵對於降低loss的貢獻。

這些方法雖然在不同的問題上都可以對特徵重要性作出評估，不過它們的缺點也是很明顯的。基於統計的方法難以捕捉到特徵之間的非線性關聯，基於模型表現的方法又或者是透過樹模型產生的重要性缺乏解釋性，我們往往只能看到有影響卻不了解怎麼影響，而線性迴歸產生出來的特徵重要線雖然可解釋性很強但卻過於簡單，只適用於線性（或是可以被線性化的問題），能處理的問題很有限。

那麼有沒有什麼特徵重要性分析方法可以有解釋性又足夠勝任應付較複雜的問題呢？  

我最近意識到，Naive Bayes或許是個好選擇！

## Naive Bayes

貝葉斯公式如下：
$$P(y|X) = \frac{P(X|y)P(y)}{P(X)}$$
其中X指的是一個向量，表示我們的特徵，y表示數據的label。我們可以具體地考慮到X的每個分量：
$$P(y|x_1, x_2, x_3, ...) = \frac{P(x_1, x_2, x_3, ...|y)P(y)}{P(x_1, x_2, x_3, ...)}$$
要舉體的求解上式是幾乎不可能的，因為我們並不知道不同的特徵之間是如何關聯的。為了解決這個問題，我們可以做一個樸素假設(Naive assumption)，即每個特徵之間是彼此獨立的，這樣上式就可以簡化為：
$$P(y|x_1, x_2, ..., x_N) = \frac{P(x_1|y) P(x_2|y)...P(x_N|y) \times P(y)}{P(x_1) P(x_2)...P(x_N)}$$

這顯然是一個過度簡化的假設了，但有趣的是在很多實際的問題上，這樣的假設下訓練出來的模型表現並不差。

有了上式，問題就變得簡單許多了。我們剩下的只要對上市中的似然函數$P(x_i|y)$的函數形式做出假設，再從數據中擬合即可。